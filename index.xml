<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Visual Media Lab on VML-BGU</title><link>https://nancyaq.github.io/vml-website/</link><description>Recent content in Visual Media Lab on VML-BGU</description><generator>Hugo</generator><language>en</language><lastBuildDate>Mon, 01 Jan 0001 00:00:00 +0000</lastBuildDate><atom:link href="https://nancyaq.github.io/vml-website/index.xml" rel="self" type="application/rss+xml"/><item><title>About Our Lab</title><link>https://nancyaq.github.io/vml-website/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nancyaq.github.io/vml-website/about/</guid><description>&lt;h1 id="welcome-to-the-visual-media-lab-vml"&gt;Welcome to the Visual Media Lab (VML)&lt;/h1&gt;
&lt;h2 id="who-we-arethis-is-all-to-be-replaced"&gt;Who We Are(THIS IS ALL TO BE REPLACED)&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;Visual Media Lab (VML)&lt;/strong&gt; at Ben-Gurion University is a research group dedicated to advancing the field of computer vision and multimedia analysis. We focus on developing innovative algorithms and systems for understanding and processing visual data across various domains.&lt;/p&gt;
&lt;h2 id="our-research-areas"&gt;Our Research Areas&lt;/h2&gt;
&lt;p&gt;We specialize in several key areas of computer vision and pattern recognition:&lt;/p&gt;</description></item><item><title>Arabic Paleography</title><link>https://nancyaq.github.io/vml-website/projects/historical-documents/arabic-paleography/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nancyaq.github.io/vml-website/projects/historical-documents/arabic-paleography/</guid><description>&lt;h3 id="overview"&gt;Overview&lt;/h3&gt;
&lt;p&gt;This is a detailed description of the Arabic Paleography sub-project&amp;hellip;&lt;/p&gt;
&lt;h3 id="methods"&gt;Methods&lt;/h3&gt;
&lt;p&gt;We use Transformers and&amp;hellip;&lt;/p&gt;
&lt;h3 id="publications"&gt;Publications&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;List of publications here&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Comprehensive Survey of 3D Human Pose Estimation</title><link>https://nancyaq.github.io/vml-website/projects/pose-estimation/survey/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nancyaq.github.io/vml-website/projects/pose-estimation/survey/</guid><description>&lt;h3 id="overview"&gt;Overview&lt;/h3&gt;
&lt;p&gt;This project involves creating the most comprehensive survey to date on 3D Human Pose Estimation methods. We systematically categorize over 200 papers published in the last five years, creating a clear taxonomy of approaches, and providing quantitative comparisons of performance across standard benchmarks.&lt;/p&gt;
&lt;h3 id="project-goals"&gt;Project Goals&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Create a hierarchical taxonomy of 3D pose estimation methods (single-view vs. multi-view, model-based vs. model-free, etc.)&lt;/li&gt;
&lt;li&gt;Provide performance comparisons on standard datasets (Human3.6M, MPI-INF-3DHP)&lt;/li&gt;
&lt;li&gt;Identify current challenges and promising future research directions&lt;/li&gt;
&lt;li&gt;Develop an interactive web-based tool for exploring the literature taxonomy&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="current-status"&gt;Current Status&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Literature Collection:&lt;/strong&gt; Complete (217 papers)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Taxonomy Development:&lt;/strong&gt; Complete&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Performance Benchmarking:&lt;/strong&gt; In progress&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Manuscript Draft:&lt;/strong&gt; In progress&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="expected-outcome"&gt;Expected Outcome&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Journal submission to &lt;em&gt;ACM Computing Surveys&lt;/em&gt; (CSUR)&lt;/li&gt;
&lt;li&gt;Release of an interactive online literature exploration platform&lt;/li&gt;
&lt;li&gt;Release of a standardized benchmarking codebase&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Contact Us</title><link>https://nancyaq.github.io/vml-website/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nancyaq.github.io/vml-website/contact/</guid><description>&lt;p&gt;Interested in collaborating on research projects or joining our lab? We&amp;rsquo;re always open to discussing new opportunities.&lt;/p&gt;
&lt;h2 id="opportunities"&gt;Opportunities&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Research Collaboration:&lt;/strong&gt; Partner with us on cutting-edge projects in computer vision.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Join Our Team:&lt;/strong&gt; We welcome motivated PhD and MSc Students.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Student Grad Projects:&lt;/strong&gt; Explore computer vision and image processing ideas for your graduation project.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="ready-to-connect"&gt;Ready to connect?&lt;/h2&gt;
&lt;p&gt;Reach out to us directly and we&amp;rsquo;ll get back to you as soon as possible.&lt;/p&gt;</description></item><item><title>Document Alignment and Reconstruction</title><link>https://nancyaq.github.io/vml-website/projects/historical-documents/alignment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nancyaq.github.io/vml-website/projects/historical-documents/alignment/</guid><description>&lt;h3 id="project-overview"&gt;Project Overview&lt;/h3&gt;
&lt;p&gt;This sub-project focuses on one of the most challenging problems in historical document analysis: reconstructing documents that have been damaged, torn, or fragmented over time. Our goal is to create a robust pipeline that can take images of fragments and automatically determine how they fit together.&lt;/p&gt;
&lt;h3 id="technical-approach"&gt;Technical Approach&lt;/h3&gt;
&lt;p&gt;We are exploring a multi-stage approach:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Feature Extraction:&lt;/strong&gt; Using deep learning models to identify unique patterns, ink strokes, and material features on the edges of fragments.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pairwise Matching:&lt;/strong&gt; Implementing a graph-based algorithm to test potential matches between fragments.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Global Optimization:&lt;/strong&gt; Solving a jigsaw-like puzzle to find the most probable overall document structure from all pairwise matches.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="challenges"&gt;Challenges&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Non-linear deformations of aged paper and parchment.&lt;/li&gt;
&lt;li&gt;Missing pieces and large gaps in the material.&lt;/li&gt;
&lt;li&gt;Fading ink and similar visual patterns across different fragments.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="next-steps"&gt;Next Steps&lt;/h3&gt;
&lt;p&gt;We are currently curating a novel dataset of artificially fragmented documents with ground truth alignment data to train and evaluate our models.&lt;/p&gt;</description></item><item><title>From Pose to Semantic Segmentation</title><link>https://nancyaq.github.io/vml-website/projects/pose-estimation/pose-seg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nancyaq.github.io/vml-website/projects/pose-estimation/pose-seg/</guid><description>&lt;h3 id="overview"&gt;Overview&lt;/h3&gt;
&lt;p&gt;This sub-project tackles the challenging problem of segmenting humans in densely crowded scenes, where individuals often occlude each other. Traditional segmentation models struggle in these scenarios. We propose a novel multi-task framework that uses estimated human pose as a strong prior to guide the segmentation network, significantly improving performance in complex environments.&lt;/p&gt;
&lt;h3 id="key-innovations"&gt;Key Innovations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pose-Guided Attention Modules:&lt;/strong&gt; Architectural components that allow the segmentation network to focus on articulated body parts.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Synthetic Crowd Generation:&lt;/strong&gt; A pipeline for creating realistic synthetic training data for crowded scenarios.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Occlusion-Robust Losses:&lt;/strong&gt; Novel loss functions designed to handle frequent occlusions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="datasets"&gt;Datasets&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;COCO-Pose&lt;/li&gt;
&lt;li&gt;CrowdPose&lt;/li&gt;
&lt;li&gt;Our synthetic VML-Crowded dataset (to be released)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="publications"&gt;Publications&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;D. Cohen, J. Smith, &amp;ldquo;Pose2Seg: Leveraging Keypoints for Human Instance Segmentation,&amp;rdquo; &lt;em&gt;IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, 2024. (Submitted)&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Hebrew Paleography and Segmentation</title><link>https://nancyaq.github.io/vml-website/projects/historical-documents/hebrew-segmentation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nancyaq.github.io/vml-website/projects/historical-documents/hebrew-segmentation/</guid><description>&lt;h3 id="project-overview"&gt;Project Overview&lt;/h3&gt;
&lt;p&gt;The goal of this project is to create accurate pixel-level segmentation of Hebrew manuscripts, distinguishing between text, decorations, annotations, and the document background. A key challenge is the complex, often degraded appearance of these historical sources.&lt;/p&gt;
&lt;h3 id="key-tasks"&gt;Key Tasks&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Text/Background Separation:&lt;/strong&gt; Using U-Net architectures to create binary masks isolating text regions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scribal Hand Identification:&lt;/strong&gt; Going beyond simple segmentation, we are training models to classify different writing styles within the same document, which is crucial for textual scholars.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Noise and Degradation Handling:&lt;/strong&gt; Developing techniques that are robust to stains, bleed-through from the other side of the page, and physical damage.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="datasets"&gt;Datasets&lt;/h3&gt;
&lt;p&gt;We are working with digitized collections from the National Library of Israel and partnering with paleographers from the Department of Jewish History to create accurate ground truth labels.&lt;/p&gt;</description></item><item><title>Meet Our Team</title><link>https://nancyaq.github.io/vml-website/team/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nancyaq.github.io/vml-website/team/</guid><description>&lt;p&gt;Welcome to the Visual Media Lab(VML) at Ben-Gurion University. Get to know the talented researchers behind our work.&lt;/p&gt;</description></item><item><title>Publications</title><link>https://nancyaq.github.io/vml-website/publications/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nancyaq.github.io/vml-website/publications/</guid><description>&lt;h1 id="publications"&gt;Publications&lt;/h1&gt;
&lt;h2 id="2024"&gt;2024&lt;/h2&gt;
&lt;h3 id="conference-papers"&gt;Conference Papers&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Nammneh, S.&lt;/strong&gt;, Madi, B., &lt;strong&gt;Atamni, N.&lt;/strong&gt;, Boardman, S., Vasyutinsky-Shapira, D., &amp;amp; &lt;strong&gt;El-Sana, J.&lt;/strong&gt; &amp;ldquo;Detecting Spiral Text Lines in Aramaic Incantation Bowls.&amp;rdquo; In &lt;em&gt;Proceedings of International Conference on Pattern Recognition&lt;/em&gt;, pages 250-264, 2024.
&lt;div class="pdf-link"&gt;
 
 &lt;a href="https://nancyaq.github.io/vml-website//papers/icpr2024_spiral.pdf" target="_blank" class="pdf-button"&gt;
 &lt;span class="pdf-icon"&gt;📄&lt;/span&gt;
 Download PDF
 &lt;/a&gt;
 &lt;/div&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Madi, B.&lt;/strong&gt;, &lt;strong&gt;Atamni, N.&lt;/strong&gt;, Tsitrinovich, V., Vasyutinsky-Shapira, D., &lt;strong&gt;El-Sana, J.&lt;/strong&gt;, &amp;amp; others. &amp;ldquo;Automated Dating of Medieval Manuscripts with a New Dataset.&amp;rdquo; In &lt;em&gt;Proceedings of International Conference on Document Analysis and Recognition&lt;/em&gt;, pages 119-139, 2024.
&lt;div class="pdf-link"&gt;
 
 &lt;a href="https://nancyaq.github.io/vml-website//papers/Automated-Dating.pdf" target="_blank" class="pdf-button"&gt;
 &lt;span class="pdf-icon"&gt;📄&lt;/span&gt;
 Download PDF
 &lt;/a&gt;
 &lt;/div&gt;&lt;/p&gt;</description></item></channel></rss>